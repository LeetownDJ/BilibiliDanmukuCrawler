import pandas as pd
import jieba
import os
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from snownlp import SnowNLP
from collections import Counter
from gensim import corpora, models
import re
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import DATA_RESULTS_DIR,FONT_PATH, STOP_WORDS_PATH


plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨é»‘ä½“æ˜¾ç¤ºä¸­æ–‡
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

def _load_stopwords():
    stopwords = set()
    if os.path.exists(STOP_WORDS_PATH):
        with open(STOP_WORDS_PATH, 'r', encoding='utf-8') as f:
            for line in f:
                stopwords.add(line.strip())
        print(f"å·²åŠ è½½åœç”¨è¯è¡¨ï¼Œå…± {len(stopwords)} ä¸ªè¯")
    else:
        print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°åœç”¨è¯æ–‡ä»¶ {STOP_WORDS_PATH}ï¼Œå°†ä¸è¿›è¡Œè¿‡æ»¤")
    return stopwords


def analyze_sentiment(file_path):
    print(f"æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ: {os.path.basename(file_path)}")
    df = pd.read_excel(file_path)

    # å®šä¹‰ç®€å•çš„æƒ…æ„Ÿè®¡ç®—å‡½æ•°
    def get_sentiment(text):
        if not isinstance(text, str) or not text:
            return 0.5
        return SnowNLP(text).sentiments

    # è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
    df['sentiment'] = df['å¼¹å¹•å†…å®¹'].astype(str).apply(get_sentiment)

    # --- ç»˜å›¾ 1: ç›´æ–¹å›¾ ---
    plt.figure(figsize=(10, 6))
    sns.histplot(df['sentiment'], bins=20, kde=True, color='skyblue')
    plt.title('å¼¹å¹•æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ (0=æ¶ˆæ, 1=ç§¯æ)')
    plt.xlabel('æƒ…æ„Ÿå¾—åˆ†')
    plt.ylabel('æ•°é‡')

    # ä¿å­˜å›¾ç‰‡
    hist_path = os.path.join(DATA_RESULTS_DIR, 'sentiment_hist.png')
    plt.savefig(hist_path)
    plt.close()  # å…³é—­ç”»å¸ƒï¼Œé‡Šæ”¾å†…å­˜
    print(f"   -> ç›´æ–¹å›¾å·²ä¿å­˜: {hist_path}")

    # --- ç»˜å›¾ 2: é¥¼å›¾ ---
    plt.figure(figsize=(8, 8))
    pos_count = len(df[df['sentiment'] > 0.5])
    neg_count = len(df[df['sentiment'] <= 0.5])

    plt.pie([pos_count, neg_count],
            labels=['ç§¯æ', 'æ¶ˆæ'],
            colors=['lightcoral', 'lightskyblue'],
            autopct='%1.1f%%',
            startangle=140)
    plt.title('å¼¹å¹•æƒ…æ„Ÿå æ¯”')

    # ä¿å­˜å›¾ç‰‡
    pie_path = os.path.join(DATA_RESULTS_DIR, 'sentiment_pie.png')
    plt.savefig(pie_path)
    plt.close()
    print(f"   -> é¥¼å›¾å·²ä¿å­˜: {pie_path}")

    # æ‰“å°å¹³å‡å€¼
    avg = df['sentiment'].mean()
    print(f"   -> å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {avg:.4f}")


def generate_wordcloud(file_path):
    print(f" æ­£åœ¨ç”Ÿæˆè¯äº‘...")
    df = pd.read_excel(file_path)
    stopwords = _load_stopwords()

    text_corpus = ' '.join(df['å¼¹å¹•å†…å®¹'].astype(str))
    seg_generator = jieba.cut(text_corpus, cut_all=False)

    # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
    seg_list = [w for w in seg_generator if len(w) > 1 and w not in stopwords]
    seg_str = ' '.join(seg_list)

    if not seg_str:
        print(" æœ‰æ•ˆè¯æ±‡ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆè¯äº‘")
        return

    # ç”Ÿæˆå¯¹è±¡
    wc = WordCloud(
        width=1000, height=600,
        background_color='white',
        font_path=FONT_PATH,  # ä» config è¯»å…¥çš„å­—ä½“è·¯å¾„
        collocations=False
    ).generate(seg_str)

    # ä¿å­˜å›¾ç‰‡
    wc_path = os.path.join(DATA_RESULTS_DIR, 'wordcloud.png')
    wc.to_file(wc_path)
    print(f"   -> è¯äº‘å›¾å·²ä¿å­˜: {wc_path}")


def count_keywords(file_path, top_n=10):
    print(f" æ­£åœ¨ç»Ÿè®¡ Top {top_n} çƒ­è¯...")
    df = pd.read_excel(file_path)
    stopwords = _load_stopwords()

    text_corpus = ' '.join(df['å¼¹å¹•å†…å®¹'].astype(str))
    words = jieba.cut(text_corpus, cut_all=False)

    # è¿‡æ»¤
    filtered_words = [w for w in words if len(w) > 1 and w not in stopwords]

    # è®¡æ•°
    counter = Counter(filtered_words)
    common_words = counter.most_common(top_n)

    print("-" * 30)
    print("çƒ­è¯æ’è¡Œæ¦œ ")
    for i, (word, count) in enumerate(common_words, 1):
        print(f"Top {i}: {word} ({count}æ¬¡)")
    print("-" * 30)


def analyze_topics(file_path, num_topics=3, num_words=5):
    """
    åŠŸèƒ½4ï¼šLDA ä¸»é¢˜æ¨¡å‹åˆ†æ (è¿›é˜¶æŒ–æ˜)
    :param num_topics: ä½ æƒ³æŠŠå¼¹å¹•åˆ†æˆå‡ ç±»ï¼Ÿ(é»˜è®¤3ç±»)
    :param num_words: æ¯ä¸ªä¸»é¢˜æ˜¾ç¤ºå‡ ä¸ªå…³é”®è¯ï¼Ÿ
    """
    print(f" æ­£åœ¨è¿›è¡Œ LDA ä¸»é¢˜æ¨¡å‹åˆ†æ (æŒ–æ˜æ·±å±‚è¯é¢˜)...")
    df = pd.read_excel(file_path)
    stopwords = _load_stopwords()

    # 1. é¢„å¤„ç†ï¼šå†æ¬¡åˆ†è¯ï¼Œå‡†å¤‡å–‚ç»™æ¨¡å‹
    # æˆ‘ä»¬éœ€è¦ä¸€ä¸ª List of List æ ¼å¼ï¼š[['è§†é¢‘', 'å¥½çœ‹'], ['å‰§æƒ…', 'ç¦»è°±'], ...]
    docs = []
    for content in df['å¼¹å¹•å†…å®¹'].astype(str):
        words = jieba.cut(content)
        # è¿‡æ»¤åœç”¨è¯ã€çŸ­è¯ã€çº¯æ•°å­—
        filtered_words = [w for w in words if len(w) > 1 and w not in stopwords and not w.isdigit()]
        if filtered_words:
            docs.append(filtered_words)

    if not docs:
        print("âš ï¸ æœ‰æ•ˆè¯æ±‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œ LDA åˆ†æ")
        return

    # 2. æ„å»ºè¯å…¸ (ç»™æ¯ä¸ªè¯ç¼–ä¸ªå·)
    dictionary = corpora.Dictionary(docs)

    # 3. æ„å»ºè¯­æ–™åº“ (æŠŠæ–‡æœ¬å˜æˆå‘é‡)
    corpus = [dictionary.doc2bow(text) for text in docs]

    # 4. è®­ç»ƒ LDA æ¨¡å‹
    # passes=10 è¡¨ç¤ºæ¨¡å‹æŠŠæ•°æ®åå¤çœ‹10éï¼Œå­¦çš„æ›´å‡†
    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)

    # 5. è¾“å‡ºç»“æœ & å¯è§†åŒ–
    print("-" * 30)
    print(f"ğŸ”¥ AI å‘ç°çš„ {num_topics} ä¸ªæ½œåœ¨è®¨è®ºä¸»é¢˜ ğŸ”¥")

    topic_data = []

    for topic_id, topic in lda_model.print_topics(num_words=num_words):
        # topic æ ¼å¼é•¿è¿™æ ·: '0.050*"å‰§æƒ…" + 0.030*"ç‰¹æ•ˆ" ...'
        # æˆ‘ä»¬ç”¨æ­£åˆ™æå–å‡ºä¸­æ–‡è¯ï¼Œæ–¹ä¾¿å±•ç¤º
        words = re.findall(r'"(.*?)"', topic)
        topic_name = f"ä¸»é¢˜ {topic_id + 1}: {', '.join(words)}"
        print(topic_name)

        # å­˜ä¸‹æ¥ç”»å›¾ç”¨
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªä¸»é¢˜æƒé‡å‡ç­‰ï¼Œæˆ–è€…ä½ å¯ä»¥åç»­æ·±å…¥æŒ–æ˜æ¯ä¸ªæ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ
        topic_data.append({'Topic': f"Topic {topic_id + 1}", 'Keywords': '\n'.join(words)})

    print("-" * 30)

    # --- ç»˜å›¾ï¼šè™½ç„¶ LDA ä¸»è¦æ˜¯çœ‹è¯ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç”»ä¸ªç®€å•çš„å…³é”®è¯å±•ç¤ºå›¾ ---
    # è¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªç®€å•çš„æ–‡æœ¬å›¾ä¿å­˜
    plt.figure(figsize=(10, 6))
    plt.axis('off')  # ä¸æ˜¾ç¤ºåæ ‡è½´
    plt.title(f'LDA æ¨¡å‹æŒ–æ˜å‡ºçš„ {num_topics} å¤§ä¸»é¢˜', fontsize=16)

    # åœ¨ç”»å¸ƒä¸Šå†™å­—
    for idx, data in enumerate(topic_data):
        plt.text(0.1, 0.8 - idx * 0.2,
                 f"{data['Topic']} (æ ¸å¿ƒè¯):",
                 fontsize=14, fontweight='bold', color='darkblue')
        plt.text(0.15, 0.75 - idx * 0.2,
                 data['Keywords'].replace('\n', ', '),
                 fontsize=12, color='dimgray')

    lda_path = os.path.join(DATA_RESULTS_DIR, 'lda_topics.png')
    plt.savefig(lda_path)
    plt.close()
    print(f"   -> ä¸»é¢˜åˆ†æå›¾å·²ä¿å­˜: {lda_path}")